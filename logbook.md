# Lesson 1
27/01/2026

Objectives, contents and motivations. Presentation of the exam modalities. 

Deep learning in artificial intelligence. Examples of deep learning applications. History of industrial revolutions. The "AI Spring". 

Introduction to machine learning: task, experience, performance measure. Expected risk.

[Introduction slides](slides/Introduction.pdf)

[Notes on the Machine Learning Framework](notes/01%20-%20The%20Machine%20Learning%20Framework.pdf)

# Lesson 2 
29/01/2026

Empirical risk. Minimization of the empirical risk on the dataset. 

Linear regression. Using the `pandas` library in Python for data manipulation. Exploring the real estate dataset.

Linear regression as empirical risk minimization. Explicit derivation of the optimal weights for linear regression (normal equations). 

[Personal notes on Linear Regression](notes/02%20-%20Linear%20regression.pdf)

[Interactive notebook on empirical risk](notebooks/01-risk.py)

[Marimo Python notebook on linear regression](notebooks/02-linear_regression.py)

[Kaggle dataset on house prices](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction)

# Lesson 3 
03/02/2026

Train and test sets. Coefficient of determination. Application to the house prices dataset.

The MNIST dataset of handwritten digits.

Introduction to binary classification. The logistic regression model. Sigmoid function. Entropy. Cross-entropy. Kullback-Leibler divergence. Jensen's inequality.

[Personal notes on Logistic Regression](notes/03%20-%20Logistic%20regression%20&%20Cross-entropy.pdf)

[Marimo Python notebook on linear regression](notebooks/02-linear_regression.py)

[Marimo Python notebook on using the MNIST dataset](notebooks/03-using_MNIST.py)

[Interactive notebook on Kullback-Leibler divergence](notebooks/04-KL.py)

[Kaggle dataset on house prices](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction)


# Lesson 4
05/02/2026

Cross-entropy loss in binary logistic regression. Optimization of the cross-entropy loss. One-hot encoding of classes. Multi-class logistic regression. Softmax function. Cross-entropy loss in multi-class logistic regression.

Multi-class logistic regression for the MNIST dataset. 

[Personal notes on Logistic Regression](notes/03%20-%20Logistic%20regression%20&%20Cross-entropy.pdf)

[Marimo Python notebook on binary logistic regression](notebooks/05-binary_logistic_regression.py)

[Marimo Python notebook on multiclass logistic regression](notebooks/06-multiclass_logistic_regression.py)

# Lesson 5 
09/02/2026

Optimization algorithms. Geometrical meaning of the gradient. Gradient descent algorithm. Continuous counterpart: gradient flow. Decrease of the loss function along a gradient flow. Comments on the choice of the learning rate with examples. 

Unrolling of the gradient descent algorithm for quadratic losses. Relationship between the learning rate and the eigenvalues of the Hessian matrix. Convergence of the gradient descent algorithm under suitable conditions on the loss function. The problem of local minima.

[Personal notes on Gradient Descent](notes/04%20-%20Gradient%20Descent.pdf)

[Interactive notebook on gradient descent](notebooks/07-gradient_descent.py)

# Lesson 6
10/02/2026

# Lesson 7
17/02/2026

# Lesson 8 
19/02/2026
