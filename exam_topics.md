This is a list of examples of topics that were not covered in the course. A possibility for the exam is to choose one of these topics, explore them in detail as we did for the topics covered in the course, and present them during the exam. 

Suggestions by the students not present in this list are welcome.

**N.B.**: In any case, we should agree on the topic before deciding for the exam.

1. More advanced optimization algorithms (momentum, RMSprop, Adam)
2. Underfitting, overfitting, bias-variance trade-off, regularization techniques ($L^2$ regularization, dropout)
3. The vanishing gradient problem, the exploding gradient problem, batch normalization, layer normalization: theoretical introduction, writing the class for the `mydl` library, testing on a dataset
4. Convolutional neural networks: theoretical introduction, writing the class for the `mydl` library, testing on a dataset
5. Recurrent neural networks and LSTMs: theoretical introduction, writing the class for the `mydl` library, testing on a dataset
6. Anomaly detection with autoencoders
7. Generative adversarial networks: theoretical introduction, example of application. WGANs?
8. Word embeddings: CBOW, Skip-gram, example of application. The Johnson-Lindenstrauss lemma? 
9. Attention mechanisms and transformers: theoretical introduction, writing the class for the `mydl` library, testing on a datase