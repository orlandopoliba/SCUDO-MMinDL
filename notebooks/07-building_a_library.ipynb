{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python library\n",
    "\n",
    "First of all, we create a new folder in our project and name it `mydl` (for \"My Deep Learning\"). Inside this folder, we create a new file named `__init__.py` (this file can be empty). This file is necessary to tell Python that the folder is a package and can be imported. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Sequential` class\n",
    "\n",
    "Inside the `mydl` folder, we create a new file named `architecture.py`. This file will contain the architecture of our neural network. We will start by creating a class named `Sequential` (this is the only one we treat in this course). A `Sequential` object is a list of layers that are executed in sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment we just define the initializer. It receives a list `layers` of layers and stores it as an attribute of the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this has no meaning if we don't have `Layer` objects. We will create them in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Layer` class\n",
    "\n",
    "We create a new file named `layers.py` inside the `mydl` folder. This file will contain the definition of the `Layer` class and its subclasses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Layer` class is an abstract class that defines the interface for all layers. Now we are interested in initializing the parameters of the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.parameters = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `Linear` class that inherits from `Layer`. We recall that a linear layer is structured in the following way:\n",
    "- it receives a tensor $x \\in \\mathbb{R}^{N \\times M_{in}}$ as input. Here, $N$ is the number of samples and $M_{in}$ is the number of input features.\n",
    "- it is defined by a weight matrix $W \\in \\mathbb{R}^{M_{in} \\times M_{out}}$ and a bias vector $b \\in R^{1 \\times M_{out}}$. Here, $M_{out}$ is the number of output features.\n",
    "- the output of the layer is given by $y = xW + b \\in \\mathbb{R}^{N \\times M_{out}}$.\n",
    "Hence, to initialize a `Linear` layer, we need to specify the number of input features (`fan_in`, using the jargon of logic gates) and the number of output features (`fan_out`). Given these two numbers, we can initialize the weight matrix $W$ with random values and the bias vector $b$ with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "  def __init__(self, fan_in, fan_out):\n",
    "    super().__init__()\n",
    "    self.parameters['W'] = torch.randn((fan_in,fan_out), dtype=torch.float32, requires_grad=False) \n",
    "    self.parameters['b'] = torch.zeros((1,fan_out), dtype=torch.float32, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: the `torch` library allows to compute the gradients of the loss function with respect to the parameters of the model. This is done by the `autograd` module. The `requires_grad` attribute of a tensor tells PyTorch to compute the gradients of the tensor with respect to the loss function. By default, this attribute is set to `True`. Here, we set it to `False`, because we want to write from scratch the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the code is working. [See the code in this other notebook.](08-testing_the_library.ipynb#importing_the_library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `forward` method\n",
    "\n",
    "The `forward` method is used to do a forward pass in a single layer of the network and also in the whole network. Let's implement it in the `Layer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.parameters = {}\n",
    "    \n",
    "  def forward(self, x):\n",
    "    raise NotImplementedError # Raising an error if the forward method is not implemented in the subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the `forward` method in the `Linear` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out):\n",
    "    super().__init__()\n",
    "    self.parameters['W'] = torch.randn((fan_in,fan_out), dtype=torch.float32, requires_grad=False) \n",
    "    self.parameters['b'] = torch.zeros((1,fan_out), dtype=torch.float32, requires_grad=False)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return x @ self.parameters['W'] + self.parameters['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this for other layers. Let us define the `Sigmoid` activation layer and its forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super().__init__() \n",
    "    \n",
    "  def forward(self, x):\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to implement the `forward` method in the `Sequential` class. This method will iterate over the layers of the network and apply the `forward` method of each layer. It will return the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "    \n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer.forward(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the code is working. [See the code in this other notebook.](08-testing_the_library.ipynb#testing_the_forward_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Loss` class \n",
    "\n",
    "We create a new file named `losses.py` inside the `mydl` folder. This file will contain the definition of the `Loss` class and its subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "  \n",
    "  def __init__(self):\n",
    "    pass # No need to initialize anything\n",
    "  \n",
    "  def __call__(self, *args, **kwds):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the `MSE` loss, i.e., Mean Squared Error. It is the error used, for example, in linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "  \n",
    "  def __call__(self, y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the code is working. [See the code in this other notebook.](08-testing_the_library.ipynb#loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The backpropagation algorithm\n",
    "\n",
    "See the [notes](../notes/08%20-%20Backpropagation.pdf) for the explanation of the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing the backward pass in the `MSE` loss. See the [notes](../notes/09%20-%20Grads%20|%20MSE.pdf) for the computation of the gradient of the mean squared error with respect to the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "  \n",
    "  def __call__(self, y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true)**2)\n",
    "  \n",
    "  def backward(self, y_pred, y_true):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    return 2*(y_pred - y_true).t()/n_samples # this is dL_dy_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the code is working. [See the code in this other notebook.](08-testing_the_library.ipynb#backward_pass_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For layers, we need to implement the `backward` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.parameters = {} dictionary\n",
    "    self.gradL_d = {} \n",
    "  \n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass through the layer.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError  \n",
    "\n",
    "  def backward(self, dL_dy):\n",
    "    raise NotImplementedError  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to implement the backward pass in the `Linear` layer. See the [notes](../notes/10%20-%20Grads%20|%20Linear%20layer.pdf) for the computation of the gradients of the loss with respect to the weights, the bias, and the inputs of the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "\n",
    "  def __init__(self, fan_in, fan_out):\n",
    "    super().__init__()\n",
    "    self.parameters['W'] = torch.randn((fan_in,fan_out), dtype=torch.float32, requires_grad=False) \n",
    "    self.parameters['b'] = torch.zeros((1,fan_out), dtype=torch.float32, requires_grad=False)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    self.x = x # Storing the input tensor for the backward pass\n",
    "    return x @ self.parameters['W'] + self.parameters['b']\n",
    "  \n",
    "  def backward(self, dL_dy):\n",
    "    self.gradL_d['W'] = (dL_dy @ self.x).t()\n",
    "    self.gradL_d['b'] = (dL_dy @ torch.ones(dL_dy.shape[1],1)).t()\n",
    "    return self.parameters['W'] @ dL_dy # this is dL_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
