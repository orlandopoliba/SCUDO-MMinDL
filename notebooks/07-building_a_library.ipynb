{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python library\n",
    "\n",
    "First of all, we create a new folder in our project and name it `mydl` (for \"My Deep Learning\"). Inside this folder, we create a new file named `__init__.py` (this file can be empty). This file is necessary to tell Python that the folder is a package and can be imported. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Sequential` class\n",
    "\n",
    "Inside the `mydl` folder, we create a new file named `architecture.py`. This file will contain the architecture of our neural network. We will start by creating a class named `Sequential` (this is the only one we treat in this course). A `Sequential` object is a list of layers that are executed in sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment we just define the initializer. It receives a list `layers` of layers and stores it as an attribute of the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this has no meaning if we don't have `Layer` objects. We will create them in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Layer` class\n",
    "\n",
    "We create a new file named `layers.py` inside the `mydl` folder. This file will contain the definition of the `Layer` class and its subclasses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Layer` class is an abstract class that defines the interface for all layers. Now we are interested in initializing the parameters of the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.parameters = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `Linear` class that inherits from `Layer`. We recall that a linear layer is structured in the following way:\n",
    "- it receives a tensor $x \\in \\mathbb{R}^{N \\times M_{in}}$ as input. Here, $N$ is the number of samples and $M_{in}$ is the number of input features.\n",
    "- it is defined by a weight matrix $W \\in \\mathbb{R}^{M_{in} \\times M_{out}}$ and a bias vector $b \\in R^{1 \\times M_{out}}$. Here, $M_{out}$ is the number of output features.\n",
    "- the output of the layer is given by $y = xW + b \\in \\mathbb{R}^{N \\times M_{out}}$.\n",
    "Hence, to initialize a `Linear` layer, we need to specify the number of input features (`fan_in`, using the jargon of logic gates) and the number of output features (`fan_out`). Given these two numbers, we can initialize the weight matrix $W$ with random values and the bias vector $b$ with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "  def __init__(self, fan_in, fan_out):\n",
    "    super().__init__()\n",
    "    self.parameters['W'] = torch.randn((fan_in,fan_out), dtype=torch.float32, requires_grad=False) \n",
    "    self.parameters['b'] = torch.zeros((1,fan_out), dtype=torch.float32, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: the `torch` library allows to compute the gradients of the loss function with respect to the parameters of the model. This is done by the `autograd` module. The `requires_grad` attribute of a tensor tells PyTorch to compute the gradients of the tensor with respect to the loss function. By default, this attribute is set to `True`. Here, we set it to `False`, because we want to write from scratch the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the code is working. [See the code in this other notebook.](08-testing_the_library.ipynb#importing_the_library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `forward` method\n",
    "\n",
    "The `forward` method is used to do a forward pass in a single layer of the network and also in the whole network. Let's implement it in the `Layer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.parameters = {}\n",
    "    \n",
    "  def forward(self, x):\n",
    "    raise NotImplementedError # Raising an error if the forward method is not implemented in the subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the `forward` method in the `Linear` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out):\n",
    "    super().__init__()\n",
    "    self.parameters['W'] = torch.randn((fan_in,fan_out), dtype=torch.float32, requires_grad=False) \n",
    "    self.parameters['b'] = torch.zeros((1,fan_out), dtype=torch.float32, requires_grad=False)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return x @ self.parameters['W'] + self.parameters['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this for other layers. Let us define the `Sigmoid` activation layer and its forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super().__init__() \n",
    "    \n",
    "  def forward(self, x):\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to implement the `forward` method in the `Sequential` class. This method will iterate over the layers of the network and apply the `forward` method of each layer. It will return the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "    \n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer.forward(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the code is working. [See the code in this other notebook.](08-testing_the_library.ipynb#testing_the_forward_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Loss` class \n",
    "\n",
    "We create a new file named `losses.py` inside the `mydl` folder. This file will contain the definition of the `Loss` class and its subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "  \n",
    "  def __init__(self):\n",
    "    pass # No need to initialize anything\n",
    "  \n",
    "  def __call__(self, *args, **kwds):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the `MSE` loss, i.e., Mean Squared Error. It is the error used, for example, in linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "  \n",
    "  def __call__(self, y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the code is working. [See the code in this other notebook.](08-testing_the_library.ipynb#loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The backpropagation algorithm\n",
    "\n",
    "See the [notes](../notes/08%20-%20Backpropagation.pdf) for the explanation of the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing the backward pass in the `MSE` loss. See the [notes](../notes/09%20-%20Grads%20-%20MSE.pdf) for the computation of the gradient of the mean squared error with respect to the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "  \n",
    "  def __call__(self, y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true)**2)\n",
    "  \n",
    "  def backward(self, y_pred, y_true):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    return 2*(y_pred - y_true).t()/n_samples # this is dL_dy_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the code is working. [See the code in this other notebook.](08-testing_the_library.ipynb#backward_pass_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For layers, we need to implement the `backward` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.parameters = {}\n",
    "    self.gradL_d = {} \n",
    "  \n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass through the layer.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError  \n",
    "\n",
    "  def backward(self, dL_dy):\n",
    "    raise NotImplementedError  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to implement the backward pass in the `Linear` layer. See the [notes](../notes/10%20-%20Grads%20|%20Linear%20layer.pdf) for the computation of the gradients of the loss with respect to the weights, the bias, and the inputs of the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "\n",
    "  def __init__(self, fan_in, fan_out):\n",
    "    super().__init__()\n",
    "    self.parameters['W'] = torch.randn((fan_in,fan_out), dtype=torch.float32, requires_grad=False) \n",
    "    self.parameters['b'] = torch.zeros((1,fan_out), dtype=torch.float32, requires_grad=False)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    self.x = x # Storing the input tensor for the backward pass\n",
    "    return x @ self.parameters['W'] + self.parameters['b']\n",
    "  \n",
    "  def backward(self, dL_dy):\n",
    "    self.gradL_d['W'] = (dL_dy @ self.x).t()\n",
    "    self.gradL_d['b'] = (dL_dy @ torch.ones(dL_dy.shape[1],1)).t()\n",
    "    return self.parameters['W'] @ dL_dy # this is dL_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to implement the `backward` method in all layers. We have written another layer, the `Sigmoid` layer. See the [notes](../notes/11%20-%20Grads%20-%20Nonlinear%20activations.pdf) for the computation of the gradients of the loss with respect to the inputs of a nonlinear activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "  def forward(self, x):\n",
    "    self.y = 1/(1+torch.exp(-x))\n",
    "    return self.y\n",
    "  \n",
    "  def backward(self, dL_dy):\n",
    "    return dL_dy * (self.y * (1-self.y)).t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass in the `Sequential` class\n",
    "\n",
    "Once we have a `backward` method in all layers, we can implement backpropagation in the `Sequential` class. The method `backward` will store the gradients of the loss with respect to the parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "    \n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer.forward(x)\n",
    "    return x\n",
    "  \n",
    "  def backward(self, y_true, loss):\n",
    "    y_pred = self.layers[-1].y # the attribute y (output) of the last layer is the final prediction\n",
    "    dL_dy = loss.backward(y_pred, y_true) # computing the differential of the loss with respect to the prediction\n",
    "    for layer in reversed(self.layers):\n",
    "      dL_dy = layer.backward(dL_dy) # storing all gradients and backpropagating the differential of the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that the backward pass in the `Sequential` class is working. [See the code in this other notebook.](08-testing_the_library.ipynb#backward_pass_sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Optimizer` class\n",
    "\n",
    "The last ingredient we need to train a neural network is a numerical optimization algorithm. For this reason we define the `Optimizer` class. We create a new file named `optimizers.py` inside the `mydl` folder. This file will contain the definition of the `Optimizer` class and its subclasses (at the moment, only `GD`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a base class `Optimizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer: \n",
    "  \n",
    "  def __init__(self):\n",
    "    pass\n",
    "  \n",
    "  def update(self): # this method will be implemented in the subclasses and has the role of updating the parameters of the model (one iteration of the optimization algorithm)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, we studied only the gradient descent algorithm. The `GD` class is initialized with the learning rate `learning_rate`. The `update` method receives the network as input and updates the parameters of the network using the gradient descent update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GD(Optimizer):\n",
    "  \n",
    "  def __init__(self, learning_rate):\n",
    "    self.learning_rate = learning_rate\n",
    "    \n",
    "  def update(self, network):\n",
    "    for layer in network.layers:\n",
    "      for key in layer.parameters.keys():\n",
    "        layer.parameters[key] -= self.learning_rate*layer.gradL_d[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if the update method of the `GD` class is working. [See the code in this other notebook.](08-testing_the_library.ipynb#optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `train` method in the `Sequential` class\n",
    "\n",
    "We have all the ingredients to train a neural network. We can implement the `train` method in the `Sequential` class.\n",
    "\n",
    "The method accepts the input data `x_train` and the target data `y_train`. It also accepts the `loss` object used as the loss function and the `optimizer` object used to update the parameters of the network for a given number of epochs `n_epochs`.\n",
    "\n",
    "It returns the loss of the network at each epoch, so that we can plot it and check if the network is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "    \n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer.forward(x)\n",
    "    return x\n",
    "  \n",
    "  def backward(self, y_true, loss):\n",
    "    y_pred = self.layers[-1].y\n",
    "    dL_dy = loss.backward(y_pred, y_true)\n",
    "    for layer in reversed(self.layers):\n",
    "      dL_dy = layer.backward(dL_dy)\n",
    "      \n",
    "  def train(self, x_train, y_train, loss, optimizer, n_epochs):\n",
    "    print('Training the network...')\n",
    "    losses_train = []\n",
    "    for epoch in range(n_epochs):\n",
    "      y_pred = self.forward(x_train)\n",
    "      current_loss = loss(y_pred, y_train)\n",
    "      losses_train.append(current_loss)\n",
    "      self.backward(y_train, loss)\n",
    "      optimizer.update(self)\n",
    "    print('Training complete.')\n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if the `train` method is working. [See the code in this other notebook.](08-testing_the_library.ipynb#training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
